### Descri√ß√£o do problema

O nosso problema consiste em  **Minimizar o Tempo Total de Viagem** dado uma rota feita por um transporte p√∫blico. √â um desafio de otimiza√ß√£o de roteamento, tipicamente modelado como um Vehicle Routing Problem (VRP).

Para isso, s√£o utilizadas inst√¢ncias constru√≠das a partir do *dataset* SUNT (Salvador Urban Network Transportation), que representa a rede de transporte urbano como um grafo de v√©rtices (paradas/esta√ß√µes) e arestas (conex√µes). A fun√ß√£o objetivo central √© a **minimiza√ß√£o do tempo total da viagem**. 

Uma solu√ß√£o para este problema √© definida como uma **rota**, que √© uma **permuta√ß√£o** dos n√≥s ativos que o agente deve percorrer, onde cada permuta√ß√£o √© uma posi√ß√£o no espa√ßo de solu√ß√µes.

- **O que √© uma rota?**
    
    > A rota √© formalmente representada como uma **permuta√ß√£o dos** n **v√©rtices** (paradas/esta√ß√µes). Isto √©, uma **sequ√™ncia ordenada de paradas/esta√ß√µes** que um ve√≠culo deve percorrer para satisfazer as demandas do problema, minimizando o custo total associado √† `matriz_tempo_viagem`.
    
    A rota de um ve√≠culo deve ser um ciclo que come√ßa e termina no dep√≥sito
    > 

### Base de Dados: SUNT

O **Salvador Urban Network Transportation (SUNT)** √© um conjunto de dados espaciotemporal coletado em Salvador entre mar√ßo de 2024 e mar√ßo de 2025, com o objetivo de apoiar a otimiza√ß√£o da mobilidade urbana. O *dataset* abrange uma √°rea de 694 km¬≤ e inclui informa√ß√µes detalhadas de tr√™s sistemas de transporte p√∫blico (√¥nibus regulares, metr√¥ e BRT), integrando dados de cerca de 700.000 passageiros e aproximadamente 2.000 ve√≠culos distribu√≠dos em 400 linhas, conectando cerca de 3.000 paradas e esta√ß√µes. O SUNT √© not√°vel pela inclus√£o inovadora de dados de passageiros ****(embarque e desembarque) e por sua granularidade temporal inferior a um minuto, o que o torna um recurso robusto para o desenvolvimento e avalia√ß√£o de m√©todos orientados a dados em Sistemas de Transporte Inteligentes (ITS).

### Fun√ß√£o objetivo

<aside>
üéØ

Minimizar Tempo total de viagem.

A Fun√ß√£o Objetivo √© calculada somando os custos temporais (`trip_time`) de todos os trechos percorridos na rota (permuta√ß√£o).

</aside>

$$
\min f(\pi) = \sum_{i=1}^{n-1} \text{tempo}(\pi_i, \pi_{i+1}) + \text{tempo}(\pi_n, dep√≥sito)
$$

- **Exemplo de c√°lculo do custo de uma rota**
    - **Matriz de tempo de viagem**
        
        A **matriz_tempo_viagem** √© lida como uma matriz de custos, onde a **linha** representa o **N√≥ de Origem** e a **coluna** representa o **N√≥ de Destino**. O valor na intersec√ß√£o √© o tempo necess√°rio para percorrer aquele trecho, simulando o atributo `trip_time` do SUNT.
        
        ![Captura de tela de 2025-12-15 22-38-37.png](attachment:e60d3483-722c-4c95-8766-ecf9eae57c45:Captura_de_tela_de_2025-12-15_22-38-37.png)
        
    
    A arquitetura multiagente proposta busca a **permuta√ß√£o √≥tima** (a rota) utilizando uma matriz que cont√©m o tempo necess√°rio para o ve√≠culo sair de uma origem i e ir para um destino j. 
    
    A fun√ß√£o objetivo √© **avaliar cada rota (**œÄ**)** que o agente encontra ou gera.
    
    Se o agente gerar uma rota œÄ=(0‚Üí1‚Üí3‚Üí4‚Üí2‚Üí0), o c√°lculo do **Tempo Total de Viagem** √© feito somando os tempos de cada trecho na matriz:
    
    Tempo¬†Total=T0,1+T1,3+T3,4+T4,2+T2,0 Tempo¬†Total=5+9+5+7+8=34¬†minutos
    
    Esta avalia√ß√£o num√©rica √© o que define o **valor da fun√ß√£o objetivo** que o agente busca minimizar.
    
    > O algoritmo MAHM/BDI tentar√° encontrar uma permuta√ß√£o diferente que resulte em um custo total menor. Se essa solu√ß√£o fosse a melhor encontrada pelo agente, ela se tornaria o `pbest`
    > 

### Ambiente = **Espa√ßo de solu√ß√µes**

Ambiente composto pelo conjunto das solu√ß√µes vi√°veis*.

$$
\mathcal{S} = \{ \pi \mid \pi \text{ √© uma permuta√ß√£o dos n√≥s ativos} \}
$$

<aside>
üéØ

Solu√ß√£o:

A solu√ß√£o √© uma rota. Isto √©, uma permuta√ß√£o* (ou sequ√™ncia ordenada) das paradas que o ve√≠culo deve visitar.

A permuta√ß√£o √≥tima √© a sequ√™ncia espec√≠fica de visita aos n√≥s ativos (paradas) que resulta no menor Tempo Total de Viagem quando somados os custos (tempos) correspondentes na `matriz_tempo_viagem`. √â essa permuta√ß√£o/sequ√™ncia que o algoritmo, atrav√©s do movimento cooperativo dos agentes (intensifica√ß√£o e diversifica√ß√£o), est√° constantemente buscando.

</aside>

- **Restri√ß√µes para a viabilidade de uma solu√ß√£o**
    
    Formalmente, uma solu√ß√£o $œÄ$ (gerada por permuta√ß√£o) √© vi√°vel se, e somente se, todos os arcos selecionados na rota obedecerem √†s restri√ß√µes:
    
    $$
    œÄ=(0,v_1,v_2,‚Ä¶,v_n,0)
    $$
    
    onde 0 √© o dep√≥sito e:
    
    - **Restri√ß√£o de Capacidade:** Ao percorrer o arco (i,j) (a aresta), a carga de passageiros (`loading`) no ve√≠culo naquele momento deve ser igual ou inferior √† capacidade m√°xima do ve√≠culo (`max_capacity`). Se a permuta√ß√£o levar a uma sobrecarga em qualquer arco, a solu√ß√£o œÄ √© considerada **invi√°vel**, mesmo que a troca dos n√≥s seja estruturalmente simples
    - Cada n√≥ ativo aparece exatamente uma vez (isto √©, s√£o visitados e atendidos exatamente uma vez)
    - A rota come√ßa e termina no dep√≥sito
    - Todos os arcos usados existem na matriz_tempo_viagem üí°
    
    <aside>
    üí°
    
    O problema √© modelado como um VRP em um **grafo completo dirigido***, onde todas as paradas s√£o mutuamente alcan√ß√°veis. As arestas representam apenas custos temporais, n√£o restri√ß√µes de conectividade. Dessa forma, a viabilidade de uma solu√ß√£o depende exclusivamente das restri√ß√µes de capacidade, e n√£o da exist√™ncia de caminhos.
    
    $G=(V,E),E=V√óV$
    
    </aside>
    

### Aquitetura BDI

Essa arquitetura √© baseada na arquitetura descrita no artigo 4, o **MAHM (Multiagent Architecture for Hybridization of Metaheuristics).** A descri√ß√£o, por√©m, segue a arquitetura BDI (Beliefs, Desire and Intentions). ****

Arquitetura baseada na Otimiza√ß√£o por Enxame de Part√≠culas (PSO), como agentes paralelos e colaborativos.

**Cren√ßas (Beliefs)**

> O conhecimento ou percep√ß√£o de um agente sobre o ambinete.
> 

No nosso caso, cada agente teria em sua base de conhecimento:

1 - Uma solu√ß√£o poss√≠vel dentro do espa√ßo de solu√ß√µes vi√°veis, que √© a melhor encontrada pelo agente at√© ent√£o (`pbest`)

2 - O valor da fun√ß√£o objetivo associado a essa solu√ß√£o. 

3 - A melhor solu√ß√£o encontrada pelo enxame, o `gbest`.  

**Desejo (Desire)**

> Os objetivos ou aspira√ß√µes do agente que servem como a principal motiva√ß√£o para a a√ß√£o. S√£o os objetivos a serem alcan√ßados.
> 

No nosso caso, cada a gente tem como desejo minimizar a fun√ß√£o objetivo. 

**Inten√ß√£o (Intention)**

> S√£o os desejos espec√≠ficos que o agente se compromete a alcan√ßar, impulsionando suas a√ß√µes e planos. Basicamente, √© quando o agente tranforma um objetivo em um plano de a√ß√£o. √â uma lista de a√ß√µes para atingir objetivos.
> 

A inten√ß√£o (o plano) do agente para fazer com que ele ‚Äúalcance‚Äù ou se aproxime do seu desejo (minimizar a fun√ß√£o objetivo) √© dividida em duas etapas: uma de diversifica√ß√£o e outra de intensifica√ß√£o. 

<aside>
üéØ

As inten√ß√µes do agente s√£o executadas com o prop√≥sito de encontrar uma nova posi√ß√£o cujo custo, determinado pela soma dos elementos da matriz, seja menor que o custo das posi√ß√µes atuais (`pbest` e `gbest`)

</aside>

Eis como o agente se movimenta:

1. **Primeira Fase: A Metaheur√≠stica inicial (O M√©todo de Decis√£o)**

Antes de o agente tentar se aproximar do grupo (pbest ou gbest), ele tenta melhorar a sua solu√ß√£o atual por conta pr√≥pria.

- O **M√©todo de Decis√£o** escolhe uma metaheur√≠stica baseada em solu√ß√£o √∫nica . Inicialmente, essa escolha pode ser aleat√≥ria.
- Essa metaheur√≠stica escolhida √© executada na posi√ß√£o atual do agente
- O resultado dessa execu√ß√£o gera uma nova solu√ß√£o. Esta nova solu√ß√£o torna-se a "Posi√ß√£o de Origem" para o operador de velocidade.
1. **Segunda Fase: O Operador de Velocidade (Path-Relinking) ‚Üí DIVERSIFICA√á√ÉO**

Agora que o agente j√° aplicou a metaheur√≠stica e tem uma "Posi√ß√£o de Origem" (esperan√ßosamente melhorada), ele precisa se mover em dire√ß√£o √† mem√≥ria/cren√ßa do enxame.

- O Path-Relinking atua como o operador de velocidade. Ele constr√≥i uma trajet√≥ria (uma sequ√™ncia de solu√ß√µes intermedi√°rias) conectando a **Posi√ß√£o de Origem** (onde o agente est√° ap√≥s a metaheur√≠stica) √† **Posi√ß√£o de Destino** (que pode ser o *pbest* ou *gbest*)
- O objetivo aqui √© a "**diversifica√ß√£o**": ao transformar a solu√ß√£o de origem na solu√ß√£o de destino, o agente explora novas √°reas do espa√ßo de busca que est√£o entre essas duas solu√ß√µes.
1. **Terceira Fase: Intensifica√ß√£o (Metaheur√≠stica Aninhada) ‚Üí INTENSIFICA√á√ÉO**
- Durante o trajeto do *path-relinking*, o agente passa por v√°rias solu√ß√µes intermedi√°rias.
- Se, durante essa travessia, for encontrada uma posi√ß√£o intermedi√°ria que seja melhor que a atual, **a trajet√≥ria √© interrompida.**.
- Nesse momento, a metaheur√≠stica mais eficiente (conforme o m√©todo de aprendizado) √© **executada novamente** a partir dessa posi√ß√£o intermedi√°ria para tentar explorar (intensificar) aquela regi√£o promissora.

> Resumo do Fluxo:

Para visualizar, o ciclo de uma itera√ß√£o no agente ocorre assim:

1. **Escolha e Execu√ß√£o inicial:** O agente escolhe uma estrat√©gia e a executa na sua solu√ß√£o atual.

2. **Defini√ß√£o do Movimento:** O resultado vira a *Origem*. O alvo (*pbest*/*gbest*) vira o *Destino*.

3. **Movimenta√ß√£o (Velocity Operator):** Inicia-se o *path-relinking* da Origem para o Destino.

4. **Interrup√ß√£o Oportunista:** Se no meio do caminho surgir algo bom, para-se o movimento e **roda-se a metaheur√≠stica**. üß†
> 

üß†¬†Aqui que entra a ‚Äúintelig√™ncia‚Äù do agente. Precisamos definir uma **Metodologia de Sele√ß√£o de Heur√≠sticas,** isto √©, dado as percep√ß√µes do agente do ambiente e de sua pr√≥pria base de cren√ßa, qual seria a melhor heur√≠stica para ser aplicada neste momento?

<aside>
üí°

Eis o que precisamos:

1 - Definir uma forma inteligente de escolher essas inten√ß√µes/metaheur√≠sticas. O agente olhar√° para um pool de metaheur√≠sticas e vai escolher a que probalisticamente pode retornar o melhor resultado. Isso ser√° feito pelo componente chamado DM ‚ÜíM√âTODO DE DECIS√ÉO
2 - Uma vez escolhida a metaheur√≠stica, precisamos avaliar essa escolha para decis√µes futuras. Isso ser√° feito pelo componente chamado LM ‚ÜíM√âTODO DE APRENDIZAGEM

</aside>

**M√©todo de Decis√£o e M√©todo de Aprendizagem**

**1. M√©todo de Decis√£o** 

O m√©todo de decis√£o √© o respons√°vel por determinar **qual estrat√©gia** o agente utilizar√° para tentar melhorar sua posi√ß√£o atual no espa√ßo de busca.

- **Sele√ß√£o de Estrat√©gia:** Em vez de usar um √∫nico algoritmo fixo, o agente possui acesso a um reposit√≥rio de estrat√©gias (pool de algoritmos), que s√£o metaheur√≠sticas baseadas em solu√ß√£o √∫nica.
- **Crit√©rio de Escolha:** A escolha n√£o √© necessariamente aleat√≥ria. O m√©todo decide qual estrat√©gia aplicar com base em crit√©rios predefinidos ou estat√≠sticos fornecidos pelo M√©todo de Aprendizado.
- **Adapta√ß√£o:** Se a estrat√©gia atual come√ßar a falhar em encontrar boas solu√ß√µes, o m√©todo de decis√£o pode troc√°-la por outra, baseando-se nas informa√ß√µes fornecidas pelo m√©todo de aprendizado.

**2. M√©todo de Aprendizado** 

O m√©todo de aprendizado √© respons√°vel por **atualizar a mem√≥ria e o hist√≥rico** (suas cren√ßas) do agente ap√≥s a execu√ß√£o das a√ß√µes, permitindo que experi√™ncias passadas influenciem a√ß√µes futuras.

- **Registro de Hist√≥rico:** Ele armazena informa√ß√µes cruciais sobre a trajet√≥ria de busca, como as solu√ß√µes visitadas, os par√¢metros utilizados e, vitalmente, as estat√≠sticas de **sucessos e falhas** das estrat√©gias aplicadas.
- **Feedback para o Sistema:** O objetivo principal deste m√©todo √© processar os resultados obtidos para "ensinar" ao agente o que funciona e o que n√£o funciona. Nos experimentos relatados no artigo 4, o m√©todo de aprendizado usou as estat√≠sticas de sucesso para aumentar a probabilidade de que estrat√©gias eficientes fossem escolhidas novamente no futuro.
- **Reposit√≥rio Centralizado:** Assim como os m√©todos de decis√£o, os m√©todos de aprendizado podem ser armazenados em um reposit√≥rio central, permitindo que diferentes agentes usem diferentes l√≥gicas de aprendizado ou compartilhem o mesmo m√©todo.

**A Intera√ß√£o entre os Dois M√©todos:**

A rela√ß√£o entre esses dois m√©todos cria um ciclo de melhoria cont√≠nua, descrito no artigo 4

1. O **M√©todo de Decis√£o** escolhe uma metaheur√≠stica e a executa.

2. O agente se move (via *path-relinking*).

3. O **M√©todo de Aprendizado** avalia o resultado: "Essa estrat√©gia melhorou a solu√ß√£o?" e atualiza a mem√≥ria do agente.

4. Na pr√≥xima itera√ß√£o, o M√©todo de Decis√£o consulta essa mem√≥ria atualizada para fazer uma escolha mais informada.